{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.7"},"colab":{"name":"Build RetinaNet.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"W3n-tHtw1OWp","colab_type":"text"},"source":["## Bonus:這裡示範如何將SSD改成類似RetinaNet的結構"]},{"cell_type":"code","metadata":{"id":"O-sFLmAT1OWq","colab_type":"code","colab":{}},"source":["from layers.box_utils import *\n","from layers import box_utils\n","from layers import Detect\n","from layers import functions\n","from layers import modules\n","from math import sqrt as sqrt\n","from itertools import product as product\n","from torch.autograd import Variable\n","from torch.autograd import Function\n","from layers.box_utils import decode, nms\n","import torch.nn as nn\n","\n","import os\n","import sys\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.backends.cudnn as cudnn\n","import torch.nn.init as init\n","import torch.utils.data as data\n","import numpy as np\n","import argparse\n","import torchvision\n","import pickle\n","import torch.nn.functional as F\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HMu54pvH1OWt","colab_type":"text"},"source":["## 建構一個有FPN+ASPP的BackBone"]},{"cell_type":"code","metadata":{"id":"1nJ_WLdv1OWu","colab_type":"code","colab":{}},"source":["class ASPP(nn.Module):\n","    \"\"\"\n","    Atrous spatial pyramid pooling (ASPP)\n","    \"\"\"\n","\n","    def __init__(self, in_ch, out_ch, rates):\n","        super(ASPP, self).__init__()\n","        for i, rate in enumerate(rates):\n","            self.add_module(\n","                \"c{}\".format(i),\n","                nn.Conv2d(in_ch, out_ch, (1,3), 1, padding=(0,rate), dilation=(1,rate), bias=True),\n","            )\n","\n","        for m in self.children():\n","            nn.init.normal_(m.weight, mean=0, std=0.01)\n","            nn.init.constant_(m.bias, 0)\n","\n","    def forward(self, x):\n","        return sum([stage(x) for stage in self.children()])\n","    \n","class Block(nn.Module):\n","    '''expand + depthwise + pointwise'''\n","    def __init__(self, in_planes, out_planes, expansion, stride):\n","        super(Block, self).__init__()\n","        self.stride = stride\n","\n","        planes = expansion * in_planes\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, stride=1, padding=0, bias=False)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=(1,5), stride=stride, padding=(0,2), groups=planes, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv3 = nn.Conv2d(planes, out_planes, kernel_size=1, stride=1, padding=0, bias=False)\n","        self.bn3 = nn.BatchNorm2d(out_planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride == 1 and in_planes != out_planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=1, padding=0, bias=False),\n","                nn.BatchNorm2d(out_planes),\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = F.relu(self.bn2(self.conv2(out)))\n","        out = self.bn3(self.conv3(out))\n","        out = out + self.shortcut(x) if self.stride==1 else out\n","        return out\n","\n","\n","class SeparableConv2d(nn.Module):\n","    def __init__(self,in_channels,out_channels,kernel_size=1,stride=1,padding=0,dilation=1,bias=False):\n","        super(SeparableConv2d,self).__init__()\n","\n","        self.conv1 = nn.Conv2d(in_channels,in_channels,kernel_size,stride,padding,dilation,groups=in_channels,bias=bias)\n","        self.pointwise = nn.Conv2d(in_channels,out_channels,1,1,0,1,1,bias=bias)\n","\n","    def forward(self,x):\n","        x = self.conv1(x)\n","        x = self.pointwise(x)\n","        return x\n","    \n","    \n","class SSD_FPN(nn.Module):\n","    # (expansion, out_planes, num_blocks, stride)\n","\n","    def __init__(self, phase,Company, num_classes=2,RGB=False):\n","        super(SSD_FPN, self).__init__()\n","        \n","    \n","        self.phase = phase\n","        self.RGB=RGB\n","        self.num_classes = num_classes\n","        self.cfg_prior =Company\n","        self.cfg=[(1,  32, 1, 1),\n","                  (4,  32, 2, 1),  # NOTE: change stride 2 -> 1 for CIFAR10\n","                  (4,  64, 3, 2),\n","                  (4,  64, 4, 1),\n","                  (4,  128, 3, 2),\n","                  (4,  128, 3, 1),\n","                  (4,  256, 1, 1)]\n","        self.priorbox = PriorBox(self.cfg_prior)\n","        self.priors = Variable(self.priorbox.forward(), volatile=True)\n","        self.depth=768\n","        # NOTE: change conv1 stride 2 -> 1 for CIFAR10\n","        if self.RGB:\n","            self.conv1 = nn.Conv2d(3, 32, kernel_size=(1,5), stride=1, padding=(0,2), bias=False,)\n","        else:\n","            self.conv1 = nn.Conv2d(1, 32, kernel_size=(1,5), stride=1, padding=(0,2), bias=False)\n","        self.bn1 = nn.BatchNorm2d(32)\n","        \n","        self.conv1_1=nn.Conv2d(32, 32, kernel_size=(3,5), stride=1, padding=(1,2), bias=False)\n","        \n","        self.bn1_1=nn.BatchNorm2d(32)\n","        \n","        self.conv1_2=nn.Conv2d(32, 64, kernel_size=(3,5), stride=1, padding=(1,2), bias=False)\n","        \n","        self.bn1_2=nn.BatchNorm2d(64)\n","        \n","        self.conv1_3=nn.Conv2d(64, 64,  kernel_size=(1,3),dilation=(1,2) , padding=(0,2), stride=1, bias=False)\n","        \n","        self.bn1_3=nn.BatchNorm2d(64)\n","        \n","        self.conv1_4=nn.Conv2d(64, 64, kernel_size=1, dilation=1, stride=1, padding=0, bias=False)\n","        \n","        self.bn1_4=nn.BatchNorm2d(64)\n","        \n","        \n","        self.layers = self._make_layers(in_planes=64)\n","        \n","        self.ASPP=ASPP(256,256,[2, 4, 6,8])\n","        self.smooth_layer=nn.Conv2d(256,256, kernel_size=(1,5), stride=1, padding=(0,2)) \n","        \n","        \n","        self.conv2 = nn.Conv2d(512, self.depth, kernel_size=1, stride=1, padding=0, bias=False,)\n","        self.bn2 = nn.BatchNorm2d(self.depth)\n","        \n","        \n","\n","        \n","        self.seprable=SeparableConv2d(self.depth,self.depth,3,1,1,1,False)\n","        self.sep_bn_1= nn.BatchNorm2d(self.depth)\n","        \n","        #self.ASPP_128=ASPP(self.depth,self.depth,[2, 4, 6])\n","        \n","        self.conv3= nn.Conv2d(self.depth, self.depth, kernel_size=(1,5), stride=1, padding=(0,2)) \n","        self.bn3=nn.BatchNorm2d(self.depth)\n","        \n","        self.seprable_2=SeparableConv2d(self.depth,self.depth,3,2,1,1,False)\n","        self.sep_bn_2=nn.BatchNorm2d(self.depth)\n","        \n","        self.conv6 = nn.Conv2d(self.depth, self.depth, kernel_size=(1,3),dilation=(1,2) , padding=(0,2))\n","        self.conv7 = nn.Conv2d( self.depth, self.depth, kernel_size=(1,5),stride=2, padding=(0,2))\n","        self.bn7=nn.BatchNorm2d(self.depth)\n","\n","        # Top layer\n","        self.toplayer = nn.Conv2d(self.depth, self.depth, kernel_size=1, stride=1, padding=0)  # Reduce channels\n","\n","        # Smooth layers\n","        self.smooth1 = nn.Conv2d(self.depth, self.depth, kernel_size=(1,5), stride=1, padding=(0,2))\n","        self.smooth2 = nn.Conv2d(self.depth, self.depth, kernel_size=(1,5), stride=1,  padding=(0,2))\n","\n","        # Lateral layers\n","        self.latlayer1 = nn.Conv2d(self.depth, self.depth, kernel_size=1, stride=1, padding=0)\n","        self.latlayer2 = nn.Conv2d( self.depth, self.depth, kernel_size=1, stride=1, padding=0)\n","        \n","        \n","                # Add localization and confidence lists\n","        '''這裡的6跟我們預設的default boxes很有關係，要跟config檔案配合'''\n","        '''默認是2個正方形，然後自己再添加，Aspect Ratio可以用Kmeans計算Ground Truth的中心'''\n","        '''ex. aspect ratio 是 [6,12,16,21] 就會有 6 個 defeault box'''\n","        self.loc = nn.ModuleList([\n","            nn.Conv2d(self.depth, 6 * 4, kernel_size=3, padding=1),\n","            nn.Conv2d(self.depth, 6 * 4, kernel_size=3, padding=1),\n","            nn.Conv2d(self.depth, 6 * 4, kernel_size=3, padding=1),\n","            nn.Conv2d(self.depth, 6 * 4, kernel_size=3, padding=1),\n","            nn.Conv2d(self.depth, 4 * 4, kernel_size=3, padding=1),\n","        ])\n","\n","        self.conf = nn.ModuleList([\n","            nn.Conv2d(self.depth, 6 * num_classes, kernel_size=3, padding=1),\n","            nn.Conv2d(self.depth, 6 * num_classes, kernel_size=3, padding=1),\n","            nn.Conv2d(self.depth, 6 * num_classes, kernel_size=3, padding=1),\n","            nn.Conv2d(self.depth, 6 * num_classes, kernel_size=3, padding=1),\n","            nn.Conv2d(self.depth, 4 * num_classes, kernel_size=3, padding=1),\n","        ])\n","\n","        if phase == 'test':\n","            self.softmax = nn.Softmax()\n","            self.detect = Detect(num_classes, 0, 200, 0.01, 0.45)\n","        \n","    def _upsample_add(self, x, y):\n","        _,_,H,W = y.size()\n","        return F.upsample(x, size=(H,W), mode='bilinear') + y\n","    \n","    def _make_layers(self, in_planes):\n","        layers = []\n","        for expansion, out_planes, num_blocks, stride in self.cfg:\n","            strides = [stride] + [1]*(num_blocks-1)\n","            for stride in strides:\n","                layers.append(Block(in_planes, out_planes, expansion, stride))\n","                in_planes = out_planes\n","        return nn.Sequential(*layers)\n","    \n","    def load_weights(self, base_file):\n","        other, ext = os.path.splitext(base_file)\n","        if ext == '.pkl' or '.pth':\n","            print('Loading weights into state dict...')\n","            self.load_state_dict(torch.load(base_file,\n","                                 map_location=lambda storage, loc: storage))\n","            print('Finished!')\n","        else:\n","            print('Sorry only .pth and .pkl files supported.')\n","\n","    def forward(self, x):\n","        \n","        out = self.bn1(F.leaky_relu(self.conv1(x),inplace=True))\n","        out = self.bn1_1(F.leaky_relu(self.conv1_1(out),inplace=True))\n","        out = F.max_pool2d(self.bn1_2(F.leaky_relu(self.conv1_2(out),inplace=True)),kernel_size=2, stride=2)\n","        out = self.bn1_3(F.leaky_relu(self.conv1_3(out),inplace=True))\n","        out = self.bn1_4(F.leaky_relu(self.conv1_4(out),inplace=True))\n","        \n","        \n","        \n","        out = self.layers(out)\n","        out_0 =self.ASPP(out)\n","        \n","        out=self.smooth_layer(out)\n","        out=torch.cat((out,out_0),dim=1)\n","        \n","        out = self.bn2(F.leaky_relu(self.conv2(out),inplace=True))\n","\n","        \n","        c3= self.sep_bn_1(F.leaky_relu(self.seprable(out))) ##第一層\n","\n","        \n","        c4= F.max_pool2d(self.bn3(F.leaky_relu(self.conv3(c3))),kernel_size=2, stride=2)##第二層 \n","        \n","        c5= self.sep_bn_2(F.leaky_relu( self.seprable_2(c4))) ##第三層 \n","        \n","        p6 = self.conv6(c5) ##第四層\n","        \n","        p7 = self.conv7(self.bn7(F.leaky_relu(p6,inplace=True))) ##第五層 \n","        \n","        # Top-down\n","        p5 = self.toplayer(c5)\n","        p4 = self._upsample_add(p5, self.latlayer1(c4))\n","        p3 = self._upsample_add(p4, self.latlayer2(c3))\n","        # Smooth\n","        p4 = self.smooth1(p4)\n","        p3 = self.smooth2(p3)\n","        \n","        sources = list()\n","        loc = list()\n","        conf = list()\n","\n","        sources.append(p3)\n","        sources.append(p4)\n","        sources.append(p5)\n","        sources.append(p6)\n","        sources.append(p7)\n","\n","        # apply multibox head to source layers\n","        for (x, l, c) in zip(sources, self.loc, self.conf):\n","            loc.append(l(x).permute(0, 2, 3, 1).contiguous())\n","            conf.append(c(x).permute(0, 2, 3, 1).contiguous())\n","\n","        loc = torch.cat([o.view(o.size(0), -1) for o in loc], 1)\n","        conf = torch.cat([o.view(o.size(0), -1) for o in conf], 1)\n","\n","        if self.phase == \"test\":\n","            output = self.detect(\n","                loc.view(loc.size(0), -1, 4),                   # loc preds\n","                self.softmax(conf.view(-1, self.num_classes)),  # conf preds\n","                self.priors                                     # default boxes\n","            )\n","        else:\n","            output = (\n","                loc.view(loc.size(0), -1, 4),\n","                conf.view(conf.size(0), -1, self.num_classes),\n","                self.priors\n","            )\n","        \n","        return  output\n","        # NOTE: change pooling kernel_size 7 -> 4 for CIFAR10"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hb3pgb3y1OW8","colab_type":"code","colab":{}},"source":["cfg={\n","    'num_classes': 21,\n","    'lr_steps': (280000, 360000, 400000),\n","    'max_iter': 400000,\n","    'feature_maps': [ 64,32, 16, 16,8],\n","    'min_dim': 512,\n","    'steps': [ 8, 16, 32,32,64 ],\n","    'min_sizes': [51.2, 102.4, 220.16, 337.92, 455.68],\n","    'max_sizes': [102.4, 220.16, 337.92, 455.68, 573.44],   \n","    'aspect_ratios': [ [6,12,16,21],[6,12,16,21], [6,12,16,21], [6,12,16,21], [6,12]],\n","    'variance': [1, 1],\n","    'clip': True,\n","    'name': 'Company',}"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sr5_YmJ91OW-","colab_type":"text"},"source":["### 'aspect_ratios' : 使用六張Feature Map，每一張上方有預設的anchor boxes，Boxes aspect ratio可以自己設定\n","### 'feature_maps' : 使用feature map大小為[64x64, 32x32, 16x16 ,16x16 , 8x8]\n","### 'min_sizes'、'max_sizes'可藉由下方算式算出，由作者自行設計\n","### 'steps' : Feature map回放回原本512*512的比例，如64要回放為512大概就是8倍\n","### 'variance' : Training 的一個trick，加速收斂，詳見：https://github.com/rykov8/ssd_keras/issues/53"]},{"cell_type":"markdown","metadata":{"id":"-QQ-abnD1OW_","colab_type":"text"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"fK9AxCvN1OXA","colab_type":"text"},"source":["## 'min_sizes'、'max_sizes' 計算"]},{"cell_type":"code","metadata":{"id":"dmmmnE8O1OXB","colab_type":"code","outputId":"76925202-521c-4d5b-d3b9-684ad744488f","colab":{}},"source":["import math\n","## source:https://blog.csdn.net/gbyy42299/article/details/81235891\n","min_dim = 512   ## 维度\n","# conv4_3 ==> 38 x 38\n","# fc7 ==> 19 x 19\n","# conv6_2 ==> 10 x 10\n","# conv7_2 ==> 5 x 5\n","# conv8_2 ==> 3 x 3\n","# conv9_2 ==> 1 x 1\n","mbox_source_layers = ['conv4_3', 'fc7', 'conv6_2', 'conv7_2', 'conv8_2', ] ## prior_box來源層，可以更改。很多改進都是基於此處的調整。\n","# in percent %\n","min_ratio = 20 ## 這裡即是論文中所說的Smin的= 0.2，Smax的= 0.9的初始值，經過下面的運算即可得到min_sizes，max_sizes。\n","max_ratio = 90\n","step = int(math.floor((max_ratio - min_ratio) / (len(mbox_source_layers) - 2)))## 取一個間距步長，即在下面用於循環給比取值時起一個間距作用。可以用一個具體的數值代替，這裡等於17。\n","min_sizes = []  ## 經過以下運算得到min_sizes和max_sizes。\n","max_sizes = []\n","for ratio in range(min_ratio, max_ratio + 1, step):\n","    ## 從min_ratio至max_ratio + 1每隔步驟= 17取一個值賦值給比。注意範圍函數的作用。\n","    ## min_sizes.append（）函數即把括號內部每次得到的值依次給了min_sizes。\n","    min_sizes.append(min_dim * ratio / 100.)\n","    max_sizes.append(min_dim * (ratio + step) / 100.)\n","min_sizes = [min_dim * 10 / 100.] + min_sizes\n","max_sizes = [min_dim * 20 / 100.] + max_sizes\n","\n","## steps: 這一步要仔細理解，即計算卷積層產生的prior_box距離原圖的步長，先驗框中心點的坐標會乘以step，\n","## 相當於從特徵映射位置映射回原圖位置，比如conv4_3輸出特徵圖大小為38 *38，而輸入的圖片為300* 300，\n","## 所以38 *8約等於300，所以映射步長為8.這是針對300* 300的訓練圖片。\n","steps = [8, 16, 32, 64, 100, 300]  \n","aspect_ratios = [[2], [2, 3], [2, 3], [2, 3], [2], [2]]\n"," \n","print('min_sizes: ',min_sizes)\n","print('max_sizes: ',max_sizes)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["min_sizes:  [51.2, 102.4, 220.16, 337.92, 455.68]\n","max_sizes:  [102.4, 220.16, 337.92, 455.68, 573.44]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"gz60w8-K1OXE","colab_type":"text"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"Xcl-0eMw1OXF","colab_type":"text"},"source":["## Default anchor boxes設計原理，看懂收穫很多\n","##### 可以理解 SSD原文中 8732個anchors是怎麼來的\n","##### 理解原本8732如何變成34048"]},{"cell_type":"code","metadata":{"id":"ALZxSi631OXG","colab_type":"code","colab":{}},"source":["class PriorBox(object):\n","    \"\"\"Compute priorbox coordinates in center-offset form for each source\n","    feature map.\n","    \"\"\"\n","    def __init__(self, cfg):\n","        super(PriorBox, self).__init__()\n","        self.image_size = cfg['min_dim']\n","        # number of priors for feature map location (either 4 or 6)\n","        self.num_priors = len(cfg['aspect_ratios'])\n","        self.variance = cfg['variance'] or [0.1]\n","        self.feature_maps = cfg['feature_maps']\n","        self.min_sizes = cfg['min_sizes']\n","        self.max_sizes = cfg['max_sizes']\n","        self.steps = cfg['steps']\n","        self.aspect_ratios = cfg['aspect_ratios']\n","        self.clip = cfg['clip']\n","        self.version = cfg['name']\n","        for v in self.variance:\n","            if v <= 0:\n","                raise ValueError('Variances must be greater than 0')\n","\n","    def forward(self):\n","        mean = []\n","        '''依照Feature map大小找出所有的pixel 中心'''\n","        '''下方這兩個loop會找出W個x軸pixel對上W個y軸pixel，假如現在是在38x38的feature map上，就會有38x38個值'''\n","        '''ex. [0,1],[0,2]..[0,37] [1,1],[1,2]..[1,37]..........[37,37]'''\n","        for k, f in enumerate(self.feature_maps):\n","            for i, j in product(range(f), repeat=2):\n","                f_k = self.image_size / self.steps[k] ## 如self.steps==8，就是先將原圖size normalize(/300)後再乘上8\n","                # unit center x,y\n","                '''中心點'''\n","                cx = (j + 0.5) / f_k\n","                cy = (i + 0.5) / f_k\n","\n","                # aspect_ratio: 1\n","                # rel size: min_size\n","                '''/self.image_size 就是在做normalization '''\n","                s_k = self.min_sizes[k]/self.image_size\n","                '''小的正方形box'''\n","                mean += [cx, cy, s_k, s_k]\n","\n","                # aspect_ratio: 1\n","                # rel size: sqrt(s_k * s_(k+1))\n","                '''大的正方形box'''\n","                s_k_prime = sqrt(s_k * (self.max_sizes[k]/self.image_size))\n","                mean += [cx, cy, s_k_prime, s_k_prime]\n","\n","                # rest of aspect ratios\n","                for ar in self.aspect_ratios[k]:\n","                    '''aspect ratio 2,3'''\n","                    mean += [cx, cy, s_k*sqrt(ar), s_k/sqrt(ar)]\n","                    '''aspect ratio 1/2,1/3，這裡先關掉，主要是看我們有沒有需要垂直的BBOX'''\n","                    '''開啟的話多一個aspect ratio 會多兩個BBOX，所以像上方原本2+4=6個10個boxes 會變成2+4*2=10個boxes'''\n","                   # mean += [cx, cy, s_k/sqrt(ar), s_k*sqrt(ar)]\n","        # back to torch land\n","        output = torch.Tensor(mean).view(-1, 4)\n","        if self.clip:\n","            output.clamp_(max=1, min=0)\n","        return output"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nBIiCaFi1OXI","colab_type":"code","colab":{}},"source":["PriorBox_Demo=PriorBox(cfg)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"okDpyIDt1OXK","colab_type":"code","outputId":"31c5fddf-c23e-47d8-a7ce-5b56ea28bcee","colab":{}},"source":["print(PriorBox_Demo.forward().shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["torch.Size([34048, 4])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"K9wlAAn11OXN","colab_type":"text"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"xD0laY_J1OXN","colab_type":"text"},"source":["## Loss 如何設計-這裡加入新觀念Focal Loss"]},{"cell_type":"markdown","metadata":{"id":"xfWUih_m1OXO","colab_type":"text"},"source":["#### OHEM v.s Focal Loss "]},{"cell_type":"markdown","metadata":{"id":"RqprliOw1OXP","colab_type":"text"},"source":["![title](OHEM&Focal.png)"]},{"cell_type":"code","metadata":{"id":"Ns2PsHLh1OXP","colab_type":"code","colab":{}},"source":["from layers.box_utils import match, log_sum_exp\n","\n","\n","class FocalLoss(nn.Module):\n","    def __init__(self, gamma=2, alpha=None, size_average=False):\n","        super(FocalLoss, self).__init__()\n","        self.gamma = gamma\n","        self.alpha = alpha\n","        if isinstance(alpha,(float,int)): self.alpha = torch.Tensor([alpha,1-alpha])\n","        if isinstance(alpha,list): self.alpha = torch.Tensor(alpha)\n","        self.size_average = size_average\n","\n","    def forward(self, input_, target):\n","        if input_.dim()>2:\n","            input_ = input_.view(input_.size(0),input_.size(1),-1)  # N,C,H,W => N,C,H*W\n","            input_ = input_.transpose(1,2)    # N,C,H*W => N,H*W,C\n","            input_ = input_.contiguous().view(-1,input_.size(2))   # N,H*W,C => N*H*W,C\n","        target = target.view(-1,1)\n","\n","        logpt = F.log_softmax(input_,dim=-1)\n","        logpt = logpt.gather(1,target)\n","        logpt = logpt.view(-1)\n","        pt = Variable(logpt.data.exp())\n","\n","        if self.alpha is not None:\n","            if self.alpha.type()!=input_.data.type():\n","                self.alpha = self.alpha.type_as(input_.data)\n","            at = self.alpha.gather(0,target.data.view(-1))\n","            logpt = logpt * Variable(at)\n","\n","        loss = -1 * (1-pt)**self.gamma * logpt\n","        if self.size_average: return loss.mean()\n","        else: return loss.sum()\n","\n","\n","\n","class MultiBoxLoss(nn.Module):\n","\n","    def __init__(self, num_classes, overlap_thresh, prior_for_matching,\n","                 bkg_label, neg_mining, neg_pos, neg_overlap, encode_target,\n","                 use_gpu=True,Focal_loss=True):\n","        super(MultiBoxLoss, self).__init__()\n","        self.use_gpu = use_gpu\n","        self.num_classes = num_classes\n","        self.threshold = overlap_thresh\n","        self.background_label = bkg_label\n","        self.encode_target = encode_target\n","        self.use_prior_for_matching = prior_for_matching\n","        self.do_neg_mining = neg_mining\n","        self.negpos_ratio = neg_pos\n","        self.neg_overlap = neg_overlap\n","        self.variance = cfg['variance']\n","        self.Focal_loss=Focal_loss\n","        self.F=FocalLoss()\n","     \n","\n","    def forward(self, predictions, targets):\n","        \"\"\"Multibox Loss\n","        Args:\n","            predictions (tuple): A tuple containing loc preds, conf preds,\n","            and prior boxes from SSD net.\n","                conf shape: torch.size(batch_size,num_priors,num_classes)\n","                loc shape: torch.size(batch_size,num_priors,4)\n","                priors shape: torch.size(num_priors,4)\n","\n","            targets (tensor): Ground truth boxes and labels for a batch,\n","                shape: [batch_size,num_objs,5] (last idx is the label).\n","        \"\"\"\n","\n","        loc_data, conf_data, priors = predictions\n","        num = loc_data.size(0)\n","        priors = priors[:loc_data.size(1), :]\n","        num_priors = (priors.size(0))\n","        num_classes = self.num_classes\n","\n","        # match priors (default boxes) and ground truth boxes\n","        loc_t = torch.Tensor(num, num_priors, 4)\n","        conf_t = torch.LongTensor(num, num_priors)\n","        for idx in range(num):\n","            truths = targets[idx][:, :-1].data\n","            labels = targets[idx][:, -1].data\n","            defaults = priors.data\n","            match(self.threshold, truths, defaults, self.variance, labels,\n","                  loc_t, conf_t, idx)\n","        if self.use_gpu:\n","            loc_t = loc_t.cuda()\n","            conf_t = conf_t.cuda()\n","        # wrap targets\n","        loc_t = Variable(loc_t, requires_grad=False)\n","        conf_t = Variable(conf_t, requires_grad=False)\n","\n","        pos = conf_t > 0\n","        num_pos = pos.sum(dim=1, keepdim=True)\n","\n","        # Localization Loss (Smooth L1)\n","        # Shape: [batch,num_priors,4]\n","        pos_idx = pos.unsqueeze(pos.dim()).expand_as(loc_data)\n","        loc_p = loc_data[pos_idx].view(-1, 4)\n","        loc_t = loc_t[pos_idx].view(-1, 4)\n","        loss_l = F.smooth_l1_loss(loc_p, loc_t, size_average=False)\n","        \n","        batch_conf = conf_data.view(-1, self.num_classes)\n","        # Compute max conf across batch for hard negative mining\n","        if self.do_neg_mining:\n","            loss_c = log_sum_exp(batch_conf) - batch_conf.gather(1, conf_t.view(-1, 1))\n","\n","            # Hard Negative Mining\n","            loss_c = loss_c.view(num, -1)\n","            loss_c[pos] = 0\n","            _, loss_idx = loss_c.sort(1, descending=True)\n","            _, idx_rank = loss_idx.sort(1)\n","            num_pos = pos.long().sum(1, keepdim=True)\n","            num_neg = torch.clamp(self.negpos_ratio*num_pos, max=pos.size(1)-1)\n","            neg = idx_rank < num_neg.expand_as(idx_rank)\n","\n","            # Confidence Loss Including Positive and Negative Examples\n","            pos_idx = pos.unsqueeze(2).expand_as(conf_data)\n","            neg_idx = neg.unsqueeze(2).expand_as(conf_data)\n","            conf_p = conf_data[(pos_idx+neg_idx).gt(0)].view(-1, self.num_classes)\n","            targets_weighted = conf_t[(pos+neg).gt(0)]\n","            loss_c = F.cross_entropy(conf_p, targets_weighted, size_average=True)\n","        else:\n","            if self.Focal_loss:\n","                loss_c=self.F(batch_conf,conf_t.view(-1))\n","            else:\n","                loss_c = F.cross_entropy(batch_conf, conf_t.view(-1), size_average=True)\n","        # Sum of losses: L(x,c,l,g) = (Lconf(x, c) + αLloc(x,l,g)) / N\n","        #double轉成torch.float64\n","        N = num_pos.data.sum().double()\n","        loss_l = loss_l.double()\n","        loss_c = loss_c.double()\n","        loss_l /= N\n","        loss_c /= N\n","        return loss_l, loss_c"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EVuFa9Yl1OXS","colab_type":"text"},"source":["## 產生我們Loss function，注意這裡的class要包含背景"]},{"cell_type":"code","metadata":{"id":"ppk1ODEM1OXS","colab_type":"code","colab":{}},"source":["Use_cuda=False\n","Use_Focal_Loss=True\n","criterion = MultiBoxLoss(21, 0.5, True, 0, False, 3, 0.5,False, Use_cuda,Use_Focal_Loss)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0FX7sREg1OXV","colab_type":"text"},"source":["----"]},{"cell_type":"markdown","metadata":{"id":"Ug3lkS9u1OXW","colab_type":"text"},"source":["## 基本設定"]},{"cell_type":"code","metadata":{"id":"4gLWsiqe1OXW","colab_type":"code","outputId":"0eacaf18-a346-4635-8400-ed24157140bc","colab":{}},"source":["RetinaNet=SSD_FPN('train',cfg,21,True)\n","net = RetinaNet"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/Users/chening/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:82: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"9GTNdI4a1OXZ","colab_type":"code","colab":{}},"source":["'''要不要使用gpu'''\n","Use_cuda=False\n","\n","'''tensor type會依照cpu或gpu有所不同'''\n","if torch.cuda.is_available():\n","    if args.cuda:\n","        torch.set_default_tensor_type('torch.cuda.FloatTensor')\n","    if not args.cuda:\n","        print(\"WARNING: It looks like you have a CUDA device, but aren't \" +\n","              \"using CUDA.\\nRun with --cuda for optimal training speed.\")\n","        torch.set_default_tensor_type('torch.FloatTensor')\n","else:\n","    torch.set_default_tensor_type('torch.FloatTensor')\n","\n","'''使用GPU時可以開啟DataParallel，但當Input是不定大小時，要關掉'''\n","if Use_cuda:\n","    net = torch.nn.DataParallel(ssd_net)\n","    cudnn.benchmark = True\n","'''使用GPU時模型要轉成cuda'''\n","if Use_cuda:\n","    net = net.cuda()\n","    \n","batch_size_=1\n","optimizer = optim.Adam(net.parameters(),lr=0.00001/batch_size_)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ny_gMoxh1OXb","colab_type":"text"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"COm_EQuE1OXc","colab_type":"text"},"source":["## 訓練"]},{"cell_type":"markdown","metadata":{"id":"PyjSd9mp1OXc","colab_type":"text"},"source":["## 這裡我們先示範輸入的 image,Label格式，真正在訓練時，準備成一樣格式即可"]},{"cell_type":"code","metadata":{"id":"kcdb0L2A1OXd","colab_type":"code","outputId":"6ee79e57-bff7-4066-f570-0418e2e868ca","colab":{}},"source":["'''輸入影像格式，假設batch size 為 4'''\n","image_in=torch.tensor(torch.rand(4,3,512,512),dtype=torch.float32)\n","'''Label格式，沒有固定長度，看圖像中有幾個label就有幾個'''\n","label_0=[[ 0.1804,  0.6076,  0.7701,  0.8485, 0.0000],\n","       [ 0.2250,  0.0000,  0.9238,  0.5641, 3.0000],\n","       [ 0.2250,  0.0000,  0.9238,  0.5641, 19.0000],\n","       [ 0.2950,  0.0000,  0.8238,  0.3641, 6.0000],]\n","label_1=[[ 0.1804,  0.6076,  0.7701,  0.8485, 13.0000],\n","       [ 0.2250,  0.0000,  0.9238,  0.5641, 11.0000],\n","       [ 0.2250,  0.0000,  0.9238,  0.5641, 7.0000],\n","       [ 0.2950,  0.0000,  0.8238,  0.3641, 5.0000],]\n","label_2=[[ 0.1804,  0.6076,  0.7701,  0.8485, 0.0000],\n","       [ 0.2250,  0.0000,  0.9238,  0.5641, 3.0000],\n","       [ 0.2250,  0.0000,  0.9238,  0.5641, 14.0000],\n","       [ 0.2950,  0.0000,  0.8238,  0.3641, 6.0000],]\n","label_3=[[ 0.1804,  0.6076,  0.7701,  0.8485, 0.0000],\n","       [ 0.2250,  0.0000,  0.9238,  0.5641, 3.0000],\n","       [ 0.2250,  0.0000,  0.9238,  0.5641, 19.0000],\n","       [ 0.2950,  0.0000,  0.8238,  0.3641, 6.0000],]"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/Users/chening/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  \n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"OLEbktg_1OXf","colab_type":"code","colab":{}},"source":["epochs=300\n","iteration=1000"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jOX2oPge1OXi","colab_type":"code","outputId":"b0489124-e553-46e8-bd7c-431021415d4d","colab":{}},"source":["for epoch in range(epochs):\n","    n=0\n","    loss_sum=[]\n","    loc_loss=[]\n","    conf_loss=[]\n","    for number__ in range(iteration) :\n","        '''要用Variable包裝tensor才能送入模型'''\n","        if Use_cuda:\n","            image_ = Variable(image_in.cuda())\n","            y = [Variable(torch.tensor(label_0).cuda(), volatile=True),Variable(torch.tensor(label_1).cuda(), \n","                volatile=True),Variable(torch.tensor(label_2).cuda(), volatile=True),Variable(torch.tensor(label_3).cuda(), volatile=True)]      \n","        else:\n","            image_ = Variable(image_in)\n","            y = [Variable(torch.tensor(label_0), volatile=True),Variable(torch.tensor(label_1), \n","                volatile=True),Variable(torch.tensor(label_2), volatile=True),Variable(torch.tensor(label_3), volatile=True)]\n","\n","        '''Forward Pass'''\n","        out = net(image_)\n","        '''Regression Loss and Classification Loss'''\n","        loss_l,loss_c = criterion(out,y )\n","        '''可以嘗試給不同權重'''\n","        loss = 10*loss_l+ loss_c\n","        '''Backward'''\n","        loss.backward()\n","\n","        loc_loss.append(loss_l.data.cpu().numpy())\n","        conf_loss.append(loss_c.data.cpu().numpy())\n","        loss_sum.append(loss.data.cpu().numpy())\n","        '''更新參數'''\n","        optimizer.step()\n","        '''清空Gradients'''\n","        optimizer.zero_grad()\n","        \n","        n+=1\n","        if n%1==0:\n","            print('BBOX Regression Loss: ', np.mean(loc_loss))\n","            print('Classification Loss: ', np.mean(conf_loss))\n","    '''儲存權重'''\n","    torch.save(ssd_net.state_dict(),'weights/Ｗeights.pth')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/Users/chening/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:14: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  \n","/Users/chening/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:15: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  from ipykernel import kernelapp as app\n","/Users/chening/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:2457: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n","  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n","/Users/chening/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:2539: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n","  \"See the documentation of nn.Upsample for details.\".format(mode))\n","/Users/chening/anaconda3/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n","  warnings.warn(warning.format(ret))\n"],"name":"stderr"},{"output_type":"stream","text":["BBOX Regression Loss:  0.1282352744986158\n","Classification Loss:  860.8162270642201\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2ENqdq7E1OXm","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}