{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t0GqZuiN1QhW"
   },
   "outputs": [],
   "source": [
    "from data import *\n",
    "from data import COCODetection\n",
    "from utils.augmentations import SSDAugmentation\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "import torch\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3JH1MJVA1QhY",
    "outputId": "5902e609-1bdc-43f4-d5e0-e8c311333e13"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=17.02s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "'''transform：自己定義預處理方式'''\n",
    "'''default會在 annotation資料夾尋找json檔案'''\n",
    "'''Image default路徑在 images/train2014/'''\n",
    "'''要先下載coco資料'''\n",
    "dataset=COCODetection(os.getcwd(),'train2014',transform=SSDAugmentation(300)) \n",
    "data_loader=torch.utils.data.DataLoader(dataset,batch_size=1)\n",
    "batch_iterator = iter(data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zn2I50dA1Qhb"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J2Cf4zEK1Qhc"
   },
   "outputs": [],
   "source": [
    "images, targets = next(batch_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dQXdTtPA1Qhe",
    "outputId": "0099fcfd-7bf3-47aa-dc32-886419492fa2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 300, 300])\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "'''Batch size, Channels, Height,Weight'''\n",
    "print(images.shape)\n",
    "print(type(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H7CzDZp41Qhh",
    "outputId": "2972bfd1-aaca-4d7c-e2df-13b6ce650b19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 5])\n",
      "tensor([[[ 0.1804,  0.6076,  0.7701,  0.8485, 52.0000],\n",
      "         [ 0.2250,  0.0000,  0.9238,  0.5641, 16.0000]]], dtype=torch.float64)\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "'''Batch size, 標注框數量, (x1,y1,x2,y2,object label)'''\n",
    "'''此時的x1,y1,x2,y2是經過Normalization，已經除以原圖得高與寬(x/寬,y/高)，縮放到0與1之間'''\n",
    "print(targets.shape)\n",
    "print(targets)\n",
    "print(type(targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "emQKTInI1Qhj"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F1YCqnpX1Qhk"
   },
   "source": [
    "## 如果要自行準備資料，只需要將 input (images, targets)整理得跟上方格式相同即可"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W6Cxvgiz1Qhk"
   },
   "source": [
    "## 前處理方法可以自訂，如果不使用pretrain weight，不一定要用一樣的前處理，最簡單如所有像素/255即可。"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "輸入資料型態.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
